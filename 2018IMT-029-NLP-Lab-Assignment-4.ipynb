{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208ee321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import requests\n",
    "import io\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "from nltk.lm import MLE\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b500e260",
   "metadata": {},
   "source": [
    "#   Basic N-gram Language Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4603f0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams and trigrams generated are as follows:\n",
      "('My', 'name')\n",
      "('name', 'is')\n",
      "('Dipika', 'Gurtu', 'I')\n",
      "('Gurtu', 'I', 'am')\n",
      "('I', 'am', 'an')\n",
      "('am', 'an', 'undergraduate')\n",
      "('an', 'undergraduate', 'student')\n",
      "\n",
      "Trigrams with padding symbols are as follows:\n",
      "('<s>', '<s>', 'My')\n",
      "('<s>', 'My', 'name')\n",
      "('My', 'name', 'is')\n",
      "('name', 'is', '</s>')\n",
      "('is', '</s>', '</s>')\n",
      "\n",
      "Generated sample N-grams of max length = 2 are as follows:\n",
      "('<s>',)\n",
      "('<s>', 'My')\n",
      "('My',)\n",
      "('My', 'name')\n",
      "('name',)\n",
      "('name', 'is')\n",
      "('is',)\n",
      "('is', '</s>')\n",
      "('</s>',)\n",
      "\n",
      "Flattened sentences with padding symbols are as follows:\n",
      "<s>\n",
      "My\n",
      "name\n",
      "is\n",
      "</s>\n",
      "<s>\n",
      "Dipika\n",
      "Gurtu\n",
      "I\n",
      "am\n",
      "an\n",
      "undergraduate\n",
      "student\n",
      "</s>\n",
      "<s>\n",
      "at\n",
      "IIITM\n",
      "Gwalior\n",
      "</s>\n",
      "\n",
      "Value of lazy iterators - train and vocab are as follows:\n",
      "Unigram and bigram training iterators:\n",
      "[('<s>',), ('<s>', 'My'), ('My',), ('My', 'name'), ('name',), ('name', 'is'), ('is',), ('is', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', 'Dipika'), ('Dipika',), ('Dipika', 'Gurtu'), ('Gurtu',), ('Gurtu', 'I'), ('I',), ('I', 'am'), ('am',), ('am', 'an'), ('an',), ('an', 'undergraduate'), ('undergraduate',), ('undergraduate', 'student'), ('student',), ('student', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', 'at'), ('at',), ('at', 'IIITM'), ('IIITM',), ('IIITM', 'Gwalior'), ('Gwalior',), ('Gwalior', '</s>'), ('</s>',)]\n",
      "\n",
      "--------******--------******---------\n",
      "Vocabulary iterator:\n",
      "['<s>', 'My', 'name', 'is', '</s>', '<s>', 'Dipika', 'Gurtu', 'I', 'am', 'an', 'undergraduate', 'student', '</s>', '<s>', 'at', 'IIITM', 'Gwalior', '</s>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = [['My', 'name', 'is'], ['Dipika', 'Gurtu', 'I', 'am', 'an', 'undergraduate', 'student'],['at','IIITM','Gwalior']] # Sample Text\n",
    "\n",
    "print(\"Bigrams and trigrams generated are as follows:\")\n",
    "print(*list(bigrams(text[0])),sep=\"\\n\")\n",
    "print(*(list(ngrams(text[1], n=3))),sep=\"\\n\")\n",
    "print()\n",
    "\n",
    "print(\"Trigrams with padding symbols are as follows:\")\n",
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=3))\n",
    "print(*list(ngrams(padded_sent, n=3)),sep=\"\\n\")\n",
    "print()\n",
    "\n",
    "print(\"Generated sample N-grams of max length = 2 are as follows:\")\n",
    "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
    "print(*list(everygrams(padded_bigrams, max_len=2)),sep=\"\\n\")\n",
    "print()\n",
    "\n",
    "print(\"Flattened sentences with padding symbols are as follows:\")\n",
    "print(*list(flatten(pad_both_ends(sent, n=2) for sent in text)),sep=\"\\n\")\n",
    "print()\n",
    "\n",
    "print(\"Value of lazy iterators - train and vocab are as follows:\")\n",
    "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n",
    "print(\"Unigram and bigram training iterators:\")\n",
    "for ngramlize_sent in training_ngrams:\n",
    "    print(list(ngramlize_sent),sep=\"\\n\")\n",
    "    print()\n",
    "print('--------******--------******---------')\n",
    "print(\"Vocabulary iterator:\")\n",
    "print(list(padded_sentences))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3637e",
   "metadata": {},
   "source": [
    "#   Training an N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77115d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words of first sentence of training corpus:\n",
      "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n",
      "\n",
      "Preview of training corpus:\n",
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                                               ADAM KILGARRIFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "Language users never choose words randomly, and language is essentially\n",
      "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
      "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
      "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
      "data, we shall (almost) always be able to establish \n",
      "None\n",
      "\n",
      "Initializing Model:\n",
      "Length of vocabulary:  0\n",
      "Fitting Model:\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1391 items>\n",
      "Length of vocabulary:  1391\n",
      "\n",
      "Preview of training corpus is as follows:\n",
      "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n",
      "\n",
      "Model output with unseen data:\n",
      "('language', 'is', 'never', 'random', '<UNK>', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "text = requests.get(url).content.decode('utf8')\n",
    "with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "    fout.write(text)\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "\n",
    "print(\"Tokenized words of first sentence of training corpus:\")\n",
    "print((tokenized_text[0]),sep=\"/n\")\n",
    "print()\n",
    "\n",
    "print(\"Preview of training corpus:\")\n",
    "print(print(text[:500]))\n",
    "print()\n",
    "\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "model = MLE(n)\n",
    "print(\"Initializing Model:\")\n",
    "print(\"Length of vocabulary: \", len(model.vocab))\n",
    "print(\"Fitting Model:\")\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)\n",
    "print(\"Length of vocabulary: \", len(model.vocab))\n",
    "print()\n",
    "\n",
    "print(\"Preview of training corpus is as follows:\")\n",
    "print(model.vocab.lookup(tokenized_text[0]))\n",
    "print()\n",
    "\n",
    "print(\"Model output with unseen data:\")\n",
    "print(model.vocab.lookup('language is never random lah .'.split()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9307001",
   "metadata": {},
   "source": [
    "# Using the N-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb671f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model with count of N-grams:\n",
      "<NgramCounter with 3 ngram orders and 19611 ngrams>\n",
      "\n",
      "count('language') =  25\n",
      "count('language is') =  11\n",
      "count('language is never') =  7\n",
      "\n",
      "P('language') =  0.003691671588895452\n",
      "P('is' | 'language') =  0.44\n",
      "P('never' | 'language is') =  0.6363636363636364\n",
      "\n",
      "P_log('language') =  -8.081510068120917\n",
      "P_log('is' | 'language') =  -1.1844245711374275\n",
      "P_log('never' | 'language is') =  -0.6520766965796932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained model with count of N-grams:\")\n",
    "print(model.counts)\n",
    "print()\n",
    "\n",
    "print(\"count('language') = \", model.counts['language'])\n",
    "print(\"count('language is') = \", model.counts[['language']]['is'])\n",
    "print(\"count('language is never') = \", model.counts[['language', 'is']]['never'])\n",
    "print()\n",
    "\n",
    "print(\"P('language') = \", model.score('language'))\n",
    "print(\"P('is' | 'language') = \", model.score('is', 'language'.split()))\n",
    "print(\"P('never' | 'language is') = \", model.score('never', 'language is'.split()))\n",
    "print()\n",
    "\n",
    "print(\"P_log('language') = \", model.logscore('language'))\n",
    "print(\"P_log('is' | 'language') = \", model.logscore('is', 'language'.split()))\n",
    "print(\"P_log('never' | 'language is') = \", model.logscore('never', 'language is'.split()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec53f1a",
   "metadata": {},
   "source": [
    "# Sentence generation using N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86d1c61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence generated using N-gram:\n",
      "['and', 'carroll', 'used', 'hypothesis', 'testing', 'has', 'been', 'used', ',', 'and', 'a', 'half', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "Generated sentence converted to human-readable form:\n",
      "and carroll used hypothesis testing has been used, and a half.\n",
      "\n",
      "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence generated using N-gram:\")\n",
    "print(model.generate(20, random_seed=7))\n",
    "print()\n",
    "\n",
    "print(model.score(\"<UNK>\")==model.score(\"lah\"))\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Generated sentence converted to human-readable form:\")\n",
    "print(generate_sent(model, 20, random_seed=7))\n",
    "print()\n",
    "\n",
    "print(model.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e277757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
